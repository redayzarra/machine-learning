{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CazISR8X_HUG"
      },
      "source": [
        "# Multiple Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiple Linear Regression** utilizes this equation for it's models and its general\n",
        "theory: ŷ = b0 + b1*x1 + b2*x2 + ... + bn*xn - where ŷ represents the dependent\n",
        "variable (what we want to predict) and the b0 represents the y-intercept. Next, the\n",
        "bn represent the slope coffiecient and the xn is every dependent variable that will\n",
        "be used in the dataset. The dependent variables (xn) are the features of the data \n",
        "and the equation will only have as many bn*xn components where n represents the \n",
        "number of features. \n",
        "\n",
        "For example, if we are growing potatoes on a farm we would need to account for the\n",
        "amount of water we are using, the fertilizer amount, the time in the sun, etc. All\n",
        "of these would be individual features that would affect the overall growth of our\n",
        "potatoes and we would set each one as a bn*xn component in our equation. So our \n",
        "equation would be: Potatoes (ŷ) = 8 tons (b0)  + 3 tons Fertilizer + 2 tons H2O etc."
      ],
      "metadata": {
        "id": "7DgMvq6-QVIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THE FIVE ASSUMPTIONS FOR LINEAR REGRESSION:**\n",
        "However, you cannot just blindly apply linear regression. You have to make sure \n",
        "that your dataset is fit for using a linear regression model. There are five main\n",
        "assumptions to make before using linear regression:"
      ],
      "metadata": {
        "id": "mdX1fwQkQJHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">1. Linearity - making sure that there is a directly proportional relationship\n",
        "  between each independent and dependent variable. There must be some form of \n",
        "  linear relationship for you to apply linear regression.\n",
        "\n",
        "  >2. Homoscedasticity - defined as equal variance. Meaning you don't want to see\n",
        "  any sort of cone-like shape forming with your dataset because that would mean \n",
        "  that variance is dependent of the independent variable (where linear regression\n",
        "  wouldn't be a good fit).\n",
        "\n",
        "  >3. Multivariate Normality - defined as normality of error distribution. Meaning,\n",
        "  if your dataset has many trends (dataset forms multiple seperate lines or \n",
        "  trends) then linear regression is not a good fit. You want your line to be \n",
        "  intersecting as many points as possible or atleast in the right ballpark.\n",
        "\n",
        "  >4. Independence - being independent of observations including no autocorrelation.\n",
        "  We don't want to see patterns in our data, the last thing we want to see is our\n",
        "  datasets following some sort of wave-like patterns because this means that our\n",
        "  independent variables (features) are not independent of each other. They are \n",
        "  somehow affecting one another which means linear regression is not an option.\n",
        "\n",
        "  >5. Lack of Multicollinearity - meaning we don't want our independent variables\n",
        "  or predictors to not be correlated with each other. If they are not correlated \n",
        "  then we can build a linear regression model for the data and it would work. But, \n",
        "  if the data's independent variables are correlated then we can't use linear models.\n",
        "\n",
        "  >While this is not one of the five assumptions, an extra check we can run when using\n",
        "linear regression is an outlier check. An outlier check would find outliers in a \n",
        "dataset and remove them from significantly affecting the linear regression model."
      ],
      "metadata": {
        "id": "wgzvApmDOojx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DUMMY VARIABLES AND THEIR TRAP:**\n",
        "\n",
        "Dummy variables are used when we encounter categorical data in our dataset. For\n",
        "example, if we are looking at the profit a company generates from spending in \n",
        "marketing, research, customer service.. we can just use the numerical values for\n",
        "our x variable in our equation: ŷ = b0 + b1*x1 + b2*x2 ... + bn*xn - however, it is\n",
        "difficult to add categorical variables such as state or region the company is \n",
        "located at. For this scenario, we use dummy variables. \n",
        "\n",
        "To use dummy variables, identify all the categories you have in your categorical\n",
        "independent variable. If there are only companies in New York and Cali then you \n",
        "would have two categories. Take these categories and create new columns for them,\n",
        "containing 1's and 0's for whether or not the data is from New York (1 if yes) or\n",
        "not (0 if no). The equation would then include:  ŷ = b0 + bn*xn + bn*Dn - where Dn\n",
        "is the dummy variables from your categorical data. \n",
        "\n",
        "However, it is not necessary to use all of your categories as dummy variables. For \n",
        "example, you can use just one dummy variable for New York (with 1's and 0's) \n",
        "because if it's not in NY then it has to be in CA. This is because we don't want\n",
        "the linear regression model to include the coefficient for CA in the y-intercept.\n",
        "When you are building a model, it is crucial to always remove one dummy variable \n",
        "because it causes multicollinearity since it creates two variables that affect \n",
        "each other in some way. If you have 9 dummy variables, you should only include 8. "
      ],
      "metadata": {
        "id": "c_H60606O6xp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BUILDING A MULTI-LINEAR REGRESSION MODEL:**\n",
        "\n",
        "You may encounter data with many independent variables, and one of the first steps\n",
        "for building a linear regression model is to decide which independent variables to\n",
        "keep and which to throw out. It is necessary to discard some of your independent \n",
        "variables (features) because the model might suffer from having these extra \n",
        "variables to account for. \n",
        "\n",
        "There are five main methods to building multiple linear regression models:"
      ],
      "metadata": {
        "id": "6RPIi7Rib6j0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " > 1. All In - these are cases where you would just blindly throw in all of your\n",
        "  variables. You would do this if you have prior knowledge that these are variables\n",
        "  you should be using. You would also be using this for preparing for backward\n",
        "  elimination.\n",
        "\n",
        "  >2. Backward Elimination:\n",
        "        a. Select a significance level to stay in the model, minimum level.\n",
        "        b. Fit the full model with all the possible predictors\n",
        "        c. Consider the predictors with the highest p-value \n",
        "        d. Remove the highest predictors\n",
        "        e. Fit the model without the variable (rebuild the whole model)\n",
        "        f. Repeat until all variables p-values are lower than the minimum level\n",
        "\n",
        "  >3. Forward Selection:\n",
        "        a. Select the maximum significance level for predictors to enter the model\n",
        "        b. Fit all the simple regression models (y ~ xn)\n",
        "        c. Select the one with the lowest p-value\n",
        "        d. Keep the variable and add on ONE other predictor\n",
        "        e. Consider the predictor model with the lowest p-value\n",
        "        f. Repeat adding variables until the p-value reaches the maximum level\n",
        "        g. Keep the previous model not the current one\n",
        "\n",
        " > 4. Bidirectional Elimination: also known as Step-Wise Regression\n",
        "        a. Select a significance level to enter and stay in the model\n",
        "        b. Perform the next step of forward selection (new variables P < SL-ENTER)\n",
        "        c. Perform ALL the steps of backward elimination (old variables P < SL-STAY)\n",
        "        d. Repeat step c every time you grow the model by a variable\n",
        "        e. No new variables can be added or no old variables can exit\n",
        "\n",
        " > 5. Score Comparison: All possible models (very resource intense)\n",
        "        a. Select a criterion of goodness of fit (e.g Akaike criterion)\n",
        "        b. Construct all possible regression models: 2^(n-1) total combinations\n",
        "        c. Select the model with the best criterion"
      ],
      "metadata": {
        "id": "hKZHXT1Zb9oT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOyqYHTk_Q57"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the libraries we need for implementing the multi-linear regression models\n",
        "with pandas, sci-kit learn, numpy, and matplotlib"
      ],
      "metadata": {
        "id": "mRXnBzUTdEKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as plt"
      ],
      "metadata": {
        "id": "yJ0z6_oHdESt"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgC61-ah_WIz"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then import the dataset and read it with the pandas library. And then using the\n",
        ".iloc method provided by the pandas library to assign the independent and dependent\n",
        "variables on the dataset. The dependent variable is usually the last column, in our\n",
        "case it is the \"Profit\" column.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5ctFtESshMIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('50_Startups.csv')"
      ],
      "metadata": {
        "id": "LmRUCEtLhMWu"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = dataset.iloc[:, :-1].values\n",
        "Y = dataset.iloc[:, -1].values"
      ],
      "metadata": {
        "id": "5H09yf1Zn1qg"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(X)"
      ],
      "metadata": {
        "id": "wDCsfSdFn4tW"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(Y)"
      ],
      "metadata": {
        "id": "GZDWFLWvoCKv"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VadrvE7s_lS9"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset contains categorical data as seen in the \"State\" column where there\n",
        "are many categories (state names) where companies are located. To address this, we\n",
        "will have to create dummy variables which will encode the categories and allow the \n",
        "model to interpret the data. \n",
        "\n",
        "We are able to use sci-kit learn to apply the one-hot encoding technique. We will \n",
        "need the ColumnTransformer class to change the columns in our dataset and then the \n",
        "OneHotEncoder to encode the categorical data. We need numpy to make sure the output\n",
        "of the .fit_transform method from the ColumnTransformer class is an array."
      ],
      "metadata": {
        "id": "Pt1KRC30mAog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "CB0fKMcImAyH"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))"
      ],
      "metadata": {
        "id": "bX4A3_WNmENn"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(X)"
      ],
      "metadata": {
        "id": "PzBBoQPhoJn3"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avoiding the Dummy Variable"
      ],
      "metadata": {
        "id": "Ig4mG40y5GnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You don't have to do this in Python because the sci-kit learn library automatically does this, however if you aren't able to use it for some reason... you can use this code to reconfigure your dataset:"
      ],
      "metadata": {
        "id": "HvQKG7oY6cMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X = X[:, 1:]"
      ],
      "metadata": {
        "id": "pDAsmMMK6dF_"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WemVnqgeA70k"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the dataset into the training and test set. We can use the sci-kit learn\n",
        "model selection library to utilize the train_test_split function."
      ],
      "metadata": {
        "id": "yKUNKzOZojNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "U0Hn1QlPoj6H"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
      ],
      "metadata": {
        "id": "XvE-_STWolaA"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(X_train)"
      ],
      "metadata": {
        "id": "LJQqzmQTooA4"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(X_test)"
      ],
      "metadata": {
        "id": "1gOFo6EiooDf"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(Y_train)"
      ],
      "metadata": {
        "id": "-45tnhVBooF4"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(Y_test)"
      ],
      "metadata": {
        "id": "BRqe03ovoog4"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Scaling\n"
      ],
      "metadata": {
        "id": "zGXw44PhkGHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no need to use feature scaling in multiple linear regression because the coefficients will balance themselves out to adjust for higher values."
      ],
      "metadata": {
        "id": "4Z_inMizkJbJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-McZVsQBINc"
      },
      "source": [
        "## Training the Multiple Linear Regression model on the Training set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dummy variable trap and step-wise regression is not necessary to implement\n",
        "multiple linear regression because the sci-kit learn class that we will use already\n",
        "avoids the dummy variable trap (where categorical data needs to be encoded and one\n",
        "category needs to be left out to avoid redundancies). The class will also perform \n",
        "step-wise regression for us and will automatically chooset the features with the\n",
        "highest p-value to predict the dependent variable."
      ],
      "metadata": {
        "id": "aFtEgcUdmV74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "nyaT5jznp9gZ"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the linear regression model which works for both simple linear regression and multiple linear regression"
      ],
      "metadata": {
        "id": "SjBYfoowqSsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pLYOaWTqBGR",
        "outputId": "8c15d7e4-4f69-4220-80a4-7d3091d11f92"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating an instance object with the Linear Regression class and then fitting the model to the training sets by adding X_train and Y_train as parameters"
      ],
      "metadata": {
        "id": "gNjssjHwqi5p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNkXL1YQBiBT"
      },
      "source": [
        "## Predicting the Test set results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there are many features on the dataset, it isn't possible to graph the model\n",
        "results so instead we will create two vectors. One will contain a sample of the \n",
        "actual profit values, or the dependent variable, while the other vector contains a\n",
        "sample of the predicted values from the model. We will compare the two vectors to\n",
        "effectively evaluate our model."
      ],
      "metadata": {
        "id": "HDWg_tSpvSXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "MS0YcIkR0-wf"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The .predict() method from the LinearRegression class predicts the profits (dependent variable) from the training features set (X_test) and stores it in Y_pred"
      ],
      "metadata": {
        "id": "Lu7o4erA0-9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(precision = 2)"
      ],
      "metadata": {
        "id": "Zq3Xw_hc2GaR"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displays any numerical value with only 2 decimals"
      ],
      "metadata": {
        "id": "kwDzR3X92Ghf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.concatenate((Y_pred.reshape(len(Y_pred),1), Y_test.reshape(len(Y_test),1)),1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69m3zJwsvTFo",
        "outputId": "050caeeb-b428-4572-a9d0-a1800217aedf"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[103015.2  103282.38]\n",
            " [132582.28 144259.4 ]\n",
            " [132447.74 146121.95]\n",
            " [ 71976.1   77798.83]\n",
            " [178537.48 191050.39]\n",
            " [116161.24 105008.31]\n",
            " [ 67851.69  81229.06]\n",
            " [ 98791.73  97483.56]\n",
            " [113969.44 110352.25]\n",
            " [167921.07 166187.94]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The concatenate function expects the tuple of arrays that we want to concatenate. Since we want to concatenate the Y_pred (or the predicted values for the depenedent variable), we will add Y_pred in the concatenate function. However, because we want to print the array vertically we can use the .reshape() method to print the vector vertically, which also takes the length of the array (because we need the number of elements in Y_pred) and the number of columns which is just 1. We apply the same .reshape method to the Y_test vector because we want to concatenate that. The concatenate function also needs an axis parameter which will be 1 because we want to concatenate horizontally."
      ],
      "metadata": {
        "id": "xr-knF856p_n"
      }
    }
  ]
}