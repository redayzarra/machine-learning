19.21 * .15
19.71 * .15
install.packages("dslabs")
install.packages("tidyverse")
library(tidyverse)
library(dslabs)
data("murders")
murders %>%
source("C:/Users/reday/Documents/harvard-r-basics/first_script.R", echo=TRUE)
setwd("~/machine-learning")
setwd("~/machine-learning/Simple-Linear-Regression")
dataset = read.csv('Salary_Data.csv')
View(dataset)
# Simple Linear Regression
rm(list = ls()) # Clears the environment
# Importing the dataset
dataset = read.csv('Salary_Data.csv')
# Splitting the dataset into training and testing sets with caTools:
install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Salary, SplitRatio = 2/3) # Split
training_set = subset(dataset, split == TRUE) # Data got split and two thirds is stored here
test_set = subset(dataset, split == FALSE) # The remaining third was stored here
# Feature Scaling with Standardization
training_set[, 2:3] = scale(training_set[, 2:3])
# Simple Linear Regression
rm(list = ls()) # Clears the environment
# Importing the dataset
dataset = read.csv('Salary_Data.csv')
# Splitting the dataset into training and testing sets with caTools:
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Salary, SplitRatio = 2/3) # Split
training_set = subset(dataset, split == TRUE) # Data got split and two thirds is stored here
test_set = subset(dataset, split == FALSE) # The remaining third was stored here
# Feature Scaling with Standardization
training_set[, 2:3] = scale(training_set[, 2:3])
?lm
regressor = lm(formula = Salary ~ YearsExperience, # The lm function is used to fit linear models, it takes parameter formula which is the Salary is proportional to YearsExperience
data = training_set)
# Simple Linear Regression
rm(list = ls()) # Clears the environment
# Importing the dataset
dataset = read.csv('Salary_Data.csv')
# Splitting the dataset into training and testing sets with caTools:
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Salary, SplitRatio = 2/3) # Split
training_set = subset(dataset, split == TRUE) # Data got split and two thirds is stored here
test_set = subset(dataset, split == FALSE) # The remaining third was stored here
# Feature Scaling with Standardization - taken care of by the simple linear regression package
regressor = lm(formula = Salary ~ YearsExperience, # The lm function is used to fit linear models, it takes parameter formula which is the Salary is proportional to YearsExperience
data = training_set)
View(regressor)
View(regressor)
summary(regressor)
summary(regressor)
View(regressor)
?predict
# Simple Linear Regression
rm(list = ls()) # Clears the environment
# Importing the dataset:
dataset = read.csv('Salary_Data.csv')
# Splitting the dataset into training and testing sets with caTools:
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Salary, SplitRatio = 2/3) # Split
training_set = subset(dataset, split == TRUE) # Data got split and two thirds is stored here
test_set = subset(dataset, split == FALSE) # The remaining third was stored here
# Feature Scaling with Standardization: taken care of by the simple linear regression package.
# Building the linear regression model using the built in simple linear regression package:
regressor = lm(formula = Salary ~ YearsExperience, # The lm function is used to fit linear models, it takes parameter formula which is the Salary is proportional to YearsExperience
data = training_set)
summary(regressor) # Code gives us valuable information on the regressor model.
# We can see in the "Signif Codes" section that there is *** next to the independent variable, meaning the there is really high statistical significance.
# A p-value less than 5% means high statistical significance, meaning the more impact the independent variable will have on the dependent variable.
# Predicting the test results
Y_pred = predict(regressor, newdata = test_set) # The predict function allows us to make predictions for any data, we have to pass in the model we built and the new data we want to predict on
Y_pred # The vector that stores our new predictions based off the test_set
library(ggplot2)
#install.packages(gglplot2) #Installs the ggplot2 library if it's not already there
library(ggplot2)
ggplot() +
geom_point(aes(x = training_set$YearsExperience, y = training_set$Salary))
geom_label(aes(x = "Years of Experience"))
ggplot() +
geom_point(aes(x = training_set$YearsExperience, y = training_set$Salary),
color = 'red')
ggplot() + # The ggplot() section goes first and then we add the individual components afterwards like so:
geom_point(aes(x = training_set$YearsExperience, y = training_set$Salary), # The geom_point function takes parameter aes which specifies what to plot on the X and Y axis
color = 'red') + # Sets the color of the data points
geom_line(aes(x = training_set$YearsExperience, y = predict(regressor, newdata = training_set)))
ggplot() + # The ggplot() section goes first and then we add the individual components afterwards like so:
geom_point(aes(x = training_set$YearsExperience, y = training_set$Salary), # The geom_point function takes parameter aes which specifies what to plot on the X and Y axis
color = 'red') + # Sets the color of the data points
geom_line(aes(x = training_set$YearsExperience, y = predict(regressor, newdata = training_set)),
color = 'blue')
source("C:/Users/reday/Documents/machine-learning/Simple-Linear-Regression/SL_Regression.R", echo=TRUE)
ggplot() +
geom_point(aes(x = test_set$YearsExperience, y = test_set$Salary),
color = 'black')
ggplot() +
geom_point(aes(x = test_set$YearsExperience, y = test_set$Salary),
color = 'black') +
geom_line(aes(x = test_set$YearsExperience, y = Y_pred),
color = 'red')
# Simple Linear Regression
rm(list = ls()) # Clears the environment
# Importing the dataset:
dataset = read.csv('Salary_Data.csv')
# Splitting the dataset into training and testing sets with caTools:
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Salary, SplitRatio = 2/3) # Split
training_set = subset(dataset, split == TRUE) # Data got split and two thirds is stored here
test_set = subset(dataset, split == FALSE) # The remaining third was stored here
# Feature Scaling with Standardization: taken care of by the simple linear regression package.
# Building the linear regression model using the built in simple linear regression package:
regressor = lm(formula = Salary ~ YearsExperience, # The lm function is used to fit linear models, it takes parameter formula which is the Salary is proportional to YearsExperience
data = training_set)
summary(regressor) # Code gives us valuable information on the regressor model.
# We can see in the "Signif Codes" section that there is *** next to the independent variable, meaning the there is really high statistical significance.
# A p-value less than 5% means high statistical significance, meaning the more impact the independent variable will have on the dependent variable.
# Predicting the test results:
Y_pred = predict(regressor, newdata = test_set) # The predict function allows us to make predictions for any data, we have to pass in the model we built and the new data we want to predict on
Y_pred # The vector that stores our new predictions based off the test_set
# Visualizing the training set results:
#install.packages("gglplot2") #Installs the ggplot2 library if it's not already there
library(ggplot2)
ggplot() + # The ggplot() section goes first and then we add the individual components afterwards like so:
geom_point(aes(x = training_set$YearsExperience, y = training_set$Salary), # The geom_point function takes parameter aes which specifies what to plot on the X and Y axis
color = 'red') + # Sets the color of the data points
geom_line(aes(x = training_set$YearsExperience, y = predict(regressor, newdata = training_set)), # The geom_line function creates our regression line, however we will specify the y-axis to be the prediction values of the training set
color = 'black')
# Visualizing the test set results:
ggplot() +
geom_point(aes(x = test_set$YearsExperience, y = test_set$Salary),
color = 'black') +
geom_line(aes(x = test_set$YearsExperience, y = Y_pred),
color = 'red')
ggplot() +
geom_point(aes(x = test_set$YearsExperience, y = test_set$Salary),
color = 'black') +
geom_line(aes(x = training_set$YearsExperience, y = Y_pred),
color = 'red')
ggplot() +
geom_point(aes(x = training_set$YearsExperience, y = test_set$Salary),
color = 'black') +
geom_line(aes(x = training_set$YearsExperience, y = Y_pred),
color = 'red')
# Simple Linear Regression
rm(list = ls()) # Clears the environment
# Importing the dataset:
dataset = read.csv('Salary_Data.csv')
# Splitting the dataset into training and testing sets with caTools:
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Salary, SplitRatio = 2/3) # Split
training_set = subset(dataset, split == TRUE) # Data got split and two thirds is stored here
test_set = subset(dataset, split == FALSE) # The remaining third was stored here
# Feature Scaling with Standardization: taken care of by the simple linear regression package.
# Building the linear regression model using the built in simple linear regression package:
regressor = lm(formula = Salary ~ YearsExperience, # The lm function is used to fit linear models, it takes parameter formula which is the Salary is proportional to YearsExperience
data = training_set)
summary(regressor) # Code gives us valuable information on the regressor model.
# We can see in the "Signif Codes" section that there is *** next to the independent variable, meaning the there is really high statistical significance.
# A p-value less than 5% means high statistical significance, meaning the more impact the independent variable will have on the dependent variable.
# Predicting the test results:
Y_pred = predict(regressor, newdata = test_set) # The predict function allows us to make predictions for any data, we have to pass in the model we built and the new data we want to predict on
Y_pred # The vector that stores our new predictions based off the test_set
# Visualizing the training set results:
#install.packages("gglplot2") #Installs the ggplot2 library if it's not already there
library(ggplot2)
ggplot() + # The ggplot() section goes first and then we add the individual components afterwards like so:
geom_point(aes(x = training_set$YearsExperience, y = training_set$Salary), # The geom_point function takes parameter aes which specifies what to plot on the X and Y axis
color = 'red') + # Sets the color of the data points
geom_line(aes(x = training_set$YearsExperience, y = predict(regressor, newdata = training_set)), # The geom_line function creates our regression line, however we will specify the y-axis to be the prediction values of the training set
color = 'black')
# Visualizing the test set results:
ggplot() +
geom_point(aes(x = training_set$YearsExperience, y = test_set$Salary),
color = 'black') +
geom_line(aes(x = training_set$YearsExperience, y = Y_pred),
color = 'red')
# Simple Linear Regression
rm(list = ls()) # Clears the environment
# Importing the dataset:
dataset = read.csv('Salary_Data.csv')
# Splitting the dataset into training and testing sets with caTools:
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Salary, SplitRatio = 2/3) # Split
training_set = subset(dataset, split == TRUE) # Data got split and two thirds is stored here
test_set = subset(dataset, split == FALSE) # The remaining third was stored here
# Feature Scaling with Standardization: taken care of by the simple linear regression package.
# Building the linear regression model using the built in simple linear regression package:
regressor = lm(formula = Salary ~ YearsExperience, # The lm function is used to fit linear models, it takes parameter formula which is the Salary is proportional to YearsExperience
data = training_set)
summary(regressor) # Code gives us valuable information on the regressor model.
# We can see in the "Signif Codes" section that there is *** next to the independent variable, meaning the there is really high statistical significance.
# A p-value less than 5% means high statistical significance, meaning the more impact the independent variable will have on the dependent variable.
# Predicting the test results:
Y_pred = predict(regressor, newdata = test_set) # The predict function allows us to make predictions for any data, we have to pass in the model we built and the new data we want to predict on
Y_pred # The vector that stores our new predictions based off the test_set
# Visualizing the training set results:
#install.packages("gglplot2") #Installs the ggplot2 library if it's not already there
library(ggplot2)
ggplot() + # The ggplot() section goes first and then we add the individual components afterwards like so:
geom_point(aes(x = training_set$YearsExperience, y = training_set$Salary), # The geom_point function takes parameter aes which specifies what to plot on the X and Y axis
color = 'red') + # Sets the color of the data points
geom_line(aes(x = training_set$YearsExperience, y = predict(regressor, newdata = training_set)), # The geom_line function creates our regression line, however we will specify the y-axis to be the prediction values of the training set
color = 'black')
# Visualizing the test set results:
ggplot() +
geom_point(aes(x = test_set$YearsExperience, y = test_set$Salary),
color = 'black') +
geom_line(aes(x = test_set$YearsExperience, y = Y_pred),
color = 'red')
# Simple Linear Regression
rm(list = ls()) # Clears the environment
# Importing the dataset:
dataset = read.csv('Salary_Data.csv')
# Splitting the dataset into training and testing sets with caTools:
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Salary, SplitRatio = 2/3) # Split
training_set = subset(dataset, split == TRUE) # Data got split and two thirds is stored here
test_set = subset(dataset, split == FALSE) # The remaining third was stored here
# Feature Scaling with Standardization: taken care of by the simple linear regression package.
# Building the linear regression model using the built in simple linear regression package:
regressor = lm(formula = Salary ~ YearsExperience, # The lm function is used to fit linear models, it takes parameter formula which is the Salary is proportional to YearsExperience
data = training_set)
summary(regressor) # Code gives us valuable information on the regressor model.
# We can see in the "Signif Codes" section that there is *** next to the independent variable, meaning the there is really high statistical significance.
# A p-value less than 5% means high statistical significance, meaning the more impact the independent variable will have on the dependent variable.
# Predicting the test results:
Y_pred = predict(regressor, newdata = test_set) # The predict function allows us to make predictions for any data, we have to pass in the model we built and the new data we want to predict on
Y_pred # The vector that stores our new predictions based off the test_set
# Visualizing the training set results:
#install.packages("gglplot2") #Installs the ggplot2 library if it's not already there
library(ggplot2)
ggplot() + # The ggplot() section goes first and then we add the individual components afterwards like so:
geom_point(aes(x = training_set$YearsExperience, y = training_set$Salary), # The geom_point function takes parameter aes which specifies what to plot on the X and Y axis
color = 'red') + # Sets the color of the data points
geom_line(aes(x = training_set$YearsExperience, y = predict(regressor, newdata = training_set)), # The geom_line function creates our regression line, however we will specify the y-axis to be the prediction values of the training set
color = 'black') +
ggtitle("Salary vs. Years of Experience - Training Set") +
xlab("Years of Experience") +
ylab("Salary")
# Visualizing the test set results:
ggplot() +
geom_point(aes(x = test_set$YearsExperience, y = test_set$Salary),
color = 'black') +
geom_line(aes(x = test_set$YearsExperience, y = Y_pred),
color = 'red') +
ggtitle("Salary vs. Years of Experience - Test Set") +
xlab("Years of Experience")
ylab("Salary")
# Simple Linear Regression
rm(list = ls()) # Clears the environment
# Importing the dataset:
dataset = read.csv('Salary_Data.csv')
# Splitting the dataset into training and testing sets with caTools:
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Salary, SplitRatio = 2/3) # Split
training_set = subset(dataset, split == TRUE) # Data got split and two thirds is stored here
test_set = subset(dataset, split == FALSE) # The remaining third was stored here
# Feature Scaling with Standardization: taken care of by the simple linear regression package.
# Building the linear regression model using the built in simple linear regression package:
regressor = lm(formula = Salary ~ YearsExperience, # The lm function is used to fit linear models, it takes parameter formula which is the Salary is proportional to YearsExperience
data = training_set)
summary(regressor) # Code gives us valuable information on the regressor model.
# We can see in the "Signif Codes" section that there is *** next to the independent variable, meaning the there is really high statistical significance.
# A p-value less than 5% means high statistical significance, meaning the more impact the independent variable will have on the dependent variable.
# Predicting the test results:
Y_pred = predict(regressor, newdata = test_set) # The predict function allows us to make predictions for any data, we have to pass in the model we built and the new data we want to predict on
Y_pred # The vector that stores our new predictions based off the test_set
# Visualizing the training set results:
#install.packages("gglplot2") #Installs the ggplot2 library if it's not already there
library(ggplot2)
ggplot() + # The ggplot() section goes first and then we add the individual components afterwards like so:
geom_point(aes(x = training_set$YearsExperience, y = training_set$Salary), # The geom_point function takes parameter aes which specifies what to plot on the X and Y axis
color = 'red') + # Sets the color of the data points
geom_line(aes(x = training_set$YearsExperience, y = predict(regressor, newdata = training_set)), # The geom_line function creates our regression line, however we will specify the y-axis to be the prediction values of the training set
color = 'black') +
ggtitle("Salary vs. Years of Experience - Training Set") +
xlab("Years of Experience") +
ylab("Salary")
# Visualizing the test set results:
ggplot() +
geom_point(aes(x = test_set$YearsExperience, y = test_set$Salary),
color = 'black') +
geom_line(aes(x = test_set$YearsExperience, y = Y_pred),
color = 'red') +
ggtitle("Salary vs. Years of Experience - Test Set") +
xlab("Years of Experience") +
ylab("Salary")
ggplot() + # The ggplot() section goes first and then we add the individual components afterwards like so:
geom_point(aes(x = training_set$YearsExperience, y = training_set$Salary), # The geom_point function takes parameter aes which specifies what to plot on the X and Y axis
color = 'red') + # Sets the color of the data points
geom_line(aes(x = training_set$YearsExperience, y = predict(regressor, newdata = training_set)), # The geom_line function creates our regression line, however we will specify the y-axis to be the prediction values of the training set
color = 'black') +
ggtitle("Salary vs. Years of Experience - Training Set") +
xlab("Years of Experience") +
ylab("Salary")
View(regressor)
regressor
names(regressor)
names(regressor$coefficients)
names(regressor$coefficients$(Intercept))
regressor["coefficients"]
coeffs <- regressor["coefficients"]
View(coeffs)
summary(coeffs)
coeffs[1]
coeffs[2]
coeffs <- regressor["coefficients"[1]]
View(coeffs)
coeffs <- coeffs[1]
coeffs
coeffs <- coeffs
ceffs
coeffs
coeffs <- coeffs$coefficients
coeffs
coeffs <- regressor["coefficients$coefficients"[1]]
coeffs
coeffs <- regressor["coefficients$coefficients"[1]]
coeffs <- coeffs$coefficients
coeffs
coeffs
coeffs <- regressor["coefficients"[1]]
coeffs <- coeffs$coefficients
coeffs
y-int <- coeffs$(Intercept)
y-int <- coeffs$(Intercept)
y-int <- coeffs[1]
y_int <- coeffs[1]
y <- coeffs[1]
y
m <- coeffs[2]
coeffs <- regressor["coefficients"[1]]
coeffs <- coeffs$coefficients
b <- coeffs[1]
m <- coeffs[2]
sprintf('The equation of the line is: y = ' m "x + "b))
m <- coeffs[2]
coeffs <- regressor["coefficients"[1]]
coeffs <- coeffs$coefficients
b <- coeffs[1]
m <- coeffs[2]
sprintf('The equation of the line is: y = ' m 'x + 'b)
coeffs <- regressor["coefficients"[1]]
coeffs <- coeffs$coefficients
b <- coeffs[1]
m <- coeffs[2]
sprintf('The equation of the line is: y = %x + %', m, b)
sprintf('The equation of the line is: y = % a x + % a', m, b)
