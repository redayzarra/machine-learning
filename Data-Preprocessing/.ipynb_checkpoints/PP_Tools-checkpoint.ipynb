{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37puETfgRzzg"
   },
   "source": [
    "# Data Preprocessing Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlsK-e1mQ09T"
   },
   "source": [
    "Data Pre-Processing is the first stage of the machine learning process. Here, we\n",
    "will:\n",
    "  1. Import the data and all necessary libraries and modules\n",
    "  2. Clean the data\n",
    "  3. Split the data into training and test sets\n",
    "\n",
    "Importing the necessary libraries and modules we need to start preprocessing \n",
    "our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoRP98MpR-qj"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2fbp5BVQ6oA"
   },
   "source": [
    "Importing the necessary libraries and modules we need to start preprocessing \n",
    "our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "NejVGKytAGrB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RopL7tUZSQkT"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDFKI-_oQ-Xy"
   },
   "source": [
    "Importing the dataset requires us to use the pandas library which will import\n",
    "the dataset in a new variable. Then we have to create the matrix of features and\n",
    "then the dependent variable vector. The features are the columns with which you\n",
    "will predict the final decision (dependent variable). So, the dependent variable\n",
    "is really what you WANT to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "IKCgpK5MCtfw"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[: , :-1].values\n",
    "Y = dataset.iloc[: , -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEmpdZVLRORw"
   },
   "source": [
    "1. Pandas reads the dataset and creates a data frame. The data frame is stored in the variable dataset. \n",
    "2. Matrix of Features (X) - Gets values from ALL ROWS, and all columns EXCEPT FOR LAST using Pandas' iloc function, which means locate indices, to single out the rows with our data and columns of features\n",
    "3. Dependent Variable (Y) - Gets values from ALL ROWS, and ONLY LAST COLUMN using Pandas' iloc function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "n1kkCkr5JCkJ"
   },
   "outputs": [],
   "source": [
    "# print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "TtARii5BJEKB"
   },
   "outputs": [],
   "source": [
    "# print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhfKXNxlSabC"
   },
   "source": [
    "## Taking care of missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfmOSNESRiTT"
   },
   "source": [
    "The dataset does indeed have missing data and values for some of the features,\n",
    "which can cause errors in our machine learning models. To address that, there\n",
    "are certain measures we can take to fix that:\n",
    "  1. Ignoring the missing data, simply removing it\n",
    "  2. Replace the missing data with the average of all the data in the column [X]\n",
    "\n",
    "Scikit-Learn is a data science library with a lot of tools, including data\n",
    "preprocessing tools that aid us in replacing missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "6MQwsQkLWtvA"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvkDMWZWRqIo"
   },
   "source": [
    "Tells the imputer that the missing values we want to replace are the empty ones, and we want to replace them with the mean of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "-04dJV0ZRlY3"
   },
   "outputs": [],
   "source": [
    "imputer.fit(X[: , 1:3])\n",
    "X[: , 1:3] = imputer.transform(X[: , 1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4uvx6vYRupI"
   },
   "source": [
    "1. The fit method from the SimpleImputer class allows to look for all the missing values in ALL ROWS and ONLY NUMERICAL COLUMNS\n",
    "2. The transform function of the imputer object adds the mean values in the missing data, as specified by fit(), into a new dataset. We are rewritting the original dataset to include the transformed complete version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "J_mA3l3nf2xI"
   },
   "outputs": [],
   "source": [
    "# print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CriG6VzVSjcK"
   },
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IounzUBCSSC4"
   },
   "source": [
    "The dataset includes categorical data that does not have a sequence of orders.\n",
    "For example, in the first column we have the names of all the countries which\n",
    "is important but has no numerical significance. We have to encode this data so\n",
    "the model can interpret it correctly, and we do that with one hot encoding.\n",
    "\n",
    "One hot coding means creating binary vectors for each different type of \n",
    "categorical data. For example, creating three different columns for 3 countries.\n",
    "The purchased column which contrains Yes and No's will be converted into binary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhSpdQWeSsFh"
   },
   "source": [
    "### Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "JBExitzk4BNA"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "r8t6CLAt8HVB"
   },
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [0])], remainder = 'passthrough')\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R86QVrXHSXfY"
   },
   "source": [
    "1. The column transformer, edits an entire column to our liking, takes two parameters... transformers - which is what we want to do, type of encoding we want to do, and on which columns. Remainders - the columns which we want to keep and don't want to apply transformations to.\n",
    "2. The Column Transformer class has a method called fit_transform which identifies the correct indices and changes it at the same time. However, the output is not an array which can be harmful for the model, so we use NumPy's array method to force the ouptut as an array. The output is a copy of X so we are reassigning it to the original X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "byFLqVrZ9Gdo"
   },
   "outputs": [],
   "source": [
    "# print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXh8oVSITIc6"
   },
   "source": [
    "### Encoding the Dependent Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8g6OSofSfzB"
   },
   "source": [
    "The dataset for the dependent variable also has to be encoded because the\n",
    "dependent variable consists of Yes's and No's which can be rewritten as 0's and \n",
    "1's for the model to better understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "DTIp9zVTKlF5"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "eemCzym2Kqb5"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNrpFObBSiXR"
   },
   "source": [
    "1. Identifies Yes's and No's\n",
    "2. Automatically locates the indices of the Yes's and No's and transforms them into a new array with 1's and 0's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Udz7hbeJLv8S"
   },
   "outputs": [],
   "source": [
    "# print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qb_vcgm3qZKW"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBiEjXLhSpVi"
   },
   "source": [
    "Splitting the dataset consists of making two seperate sets - one training set and\n",
    "one test set which will be used for evaluation and depolyment. Splitting the data\n",
    "is done before feature scaling, scaling all features to make them even, because\n",
    "the test set is supposed to be a brand new set to which you should be evaluating\n",
    "your machine learning model. You MUST treat it like a brand new dataset so you\n",
    "cannot apply the same feature scaling to the testing set to prevent information\n",
    "leakage.\n",
    "\n",
    "We will use the train_test_split function from scikit-learn to divide the data\n",
    "into four sections - features and dependent variable sets for both the training \n",
    "and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "tp3kxsP-QPEd"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "t5iDk-JnWuqR"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPB7ib0HStJB"
   },
   "source": [
    "Assigning the different training and testing sets of both the features and dependent variable into divided sections of the data. \n",
    "\n",
    "We are dividing the X (features) and Y (dependent variable) randomly with a seed of 1 so we can replicate our results. And making sure the size of the testing set is 20% meaning training set is 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "qiMf6AKZW91u"
   },
   "outputs": [],
   "source": [
    "# print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "7Lx9bgdGW97K"
   },
   "outputs": [],
   "source": [
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "jUeDkgDwW-B9"
   },
   "outputs": [],
   "source": [
    "# print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "hFujB23DW-IV"
   },
   "outputs": [],
   "source": [
    "# print(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpGqbS4TqkIR"
   },
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0DNQkMOTjsR"
   },
   "source": [
    "Feature scaling allows us to scale our features evenly so some features do not\n",
    "dominate other features (outliers). However, feature scaling is often unncessary\n",
    "for most machine learning models because the models use linear regression \n",
    "equations, for example: y = b0 + b1x1 + b2x2 + ... , and so the model adjusts\n",
    "for the features by choosing a smaller coefficient.\n",
    "\n",
    "There are two main feature scaling techniques that put all features on the same\n",
    "scale: standardization and normalization. Standardization is subtracting each \n",
    "value of the feature by the mean of the data and dividing the result by the \n",
    "standard deviation (results will be between -3 and 3). Normalization is when you \n",
    "subtract the minimum feature value from each feature value and then divide the \n",
    "result by the difference between the maximum and minimum feature value (results\n",
    "will be between 0 and 1).\n",
    "\n",
    "Normalization is recommended for a normal distribution in most of your features. \n",
    "Standardization is well-suited for most datasets and will work almost always \n",
    "which is why standardization is most-recommended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "OYBQPIn4tGg0"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "gLesDYbqtKB-"
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train[: , 3:] = sc.fit_transform(X_train[: , 3:])\n",
    "X_test[: , 3:] = sc.transform(X_test[: , 3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZJK_qnnTmqH"
   },
   "source": [
    "1. Create an instance object from the StandardScaler class\n",
    "2. Fit runs the scaling calculations while transform applies the results. We will use fit_transform here on the selected columns (because we don't want to apply feature scaling to our encoded variables).\n",
    "3. Since X_test will be used as the testing dataset, we want the same scalar applied to X_test like we did with X_training. Which is why we will only transform the dataset and not fit it, which would re-evaluate the scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "Admdn5XPtdXN"
   },
   "outputs": [],
   "source": [
    "# print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "WyXkrt_Vtdd2"
   },
   "outputs": [],
   "source": [
    "# print(X_train)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
